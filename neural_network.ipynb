{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02 Part 2: Neural Net Template\n",
    "\n",
    "Allan Calderwood - 202077625\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "\n",
    "    #==========================================#\n",
    "    # The init method is called when an object #\n",
    "    # is created. It can be used to initialize #\n",
    "    # the attributes of the class.             #\n",
    "    #==========================================#\n",
    "    def __init__(self, no_inputs, no_hidden_layers=1, hidden_layer_size=16, output_layer_size=1,\n",
    "                 max_iterations=5, activation=\"sigmoid\", learning_rate=0.1):\n",
    "\n",
    "        self.no_inputs = no_inputs\n",
    "        self.no_hidden_layers = no_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.output_layer_size = output_layer_size\n",
    "        self.activation = activation\n",
    "        self.max_iterations = max_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Draw initial random weights from a normal distribution, \n",
    "        # with a mean of 0 and sd of 2/No inputs for each layer, using He Initialisation,\n",
    "        # as it works well with ReLU which will be added later to try and mitigate the vanishing \n",
    "        # gradient problem\n",
    "        self.layers = []\n",
    "\n",
    "        for l in range(no_hidden_layers+2):\n",
    "            # if input layer use no_inputs to initialise, else use previous layer output size\n",
    "            if l==0: \n",
    "                # define the shape of the weights, the no. of inputs to the layer and the no. of neurons\n",
    "                weights = np.ones(shape=(self.hidden_layer_size, self.no_inputs+1))\n",
    "                inputs = no_inputs\n",
    "                n_neurons = hidden_layer_size\n",
    "            # if output layer\n",
    "            elif l==no_hidden_layers+1:\n",
    "                weights = np.ones(shape=(self.output_layer_size, self.hidden_layer_size+1))\n",
    "                inputs = hidden_layer_size\n",
    "                n_neurons = output_layer_size\n",
    "            # if hidden layer\n",
    "            else: \n",
    "                weights = np.ones(shape=(self.hidden_layer_size, self.hidden_layer_size+1))\n",
    "                inputs = hidden_layer_size\n",
    "                n_neurons = hidden_layer_size\n",
    "            # now create the weights using He initialisation\n",
    "            for x in range(n_neurons):\n",
    "                weights[x] = np.random.normal(0, (2/inputs), inputs+1)\n",
    "                \n",
    "            # add weights as a layer   \n",
    "            self.layers.insert(l, weights)\n",
    "            \n",
    "     #=======================================#\n",
    "    # Prints the details of the neural net.  #\n",
    "    #=======================================#\n",
    "    def print_details(self):\n",
    "        print(\"Model Summary\")\n",
    "        print(\"----------------------------\")\n",
    "        print(\"No. inputs: \" + str(self.no_inputs))\n",
    "        print(\"Hidden Layers: (\" + str(self.no_hidden_layers) \n",
    "              +\",\"+str(self.hidden_layer_size)+\")\")\n",
    "        print(\"Max iterations:\\t\" + str(self.max_iterations))\n",
    "        print(\"Learning rate:\\t\" + str(self.learning_rate))\n",
    "        print(\"Activation function: \" + str(self.activation))\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    #===================================#\n",
    "    # Performs the activation function. #\n",
    "    # Expects an array of values of     #\n",
    "    # shape (1,N) where N is the number #\n",
    "    # of nodes in the layer.            #\n",
    "    #===================================#\n",
    "    def activate(self, z):\n",
    "        if (self.activation==\"relu\"):\n",
    "            return z * (z > 0)\n",
    "        elif (self.activation==\"sigmoid\"):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif (self.activation==\"linear\"):\n",
    "            return z\n",
    "        \n",
    "    #===================================#\n",
    "    # Returns the derivatives of the    #\n",
    "    # activation function chosen        #\n",
    "    #===================================#    \n",
    "    def derivative(self, z):\n",
    "        if (self.activation==\"relu\"):\n",
    "            return 1.*(z > 0)\n",
    "        elif (self.activation==\"sigmoid\"):\n",
    "            return z*(1.0-z)\n",
    "        elif (self.activation==\"linear\"):\n",
    "            return 1\n",
    "    \n",
    "    #===================================#\n",
    "    # Loss function of the neural net   #\n",
    "    #===================================#    \n",
    "    def loss(self, output, actual):\n",
    "        return np.array(output-actual)\n",
    "    \n",
    "    #===================================#\n",
    "    # Performs the forward phase of the #\n",
    "    # Neural network                    #\n",
    "    #===================================#  \n",
    "    def forward_phase(self, inputs):\n",
    "        instance =  inputs\n",
    "        # add 1 to training data for use with bias\n",
    "        instance = np.hstack(([1], instance))\n",
    "        # get output of first layer and store in 2d array--\n",
    "        self.outputs = []\n",
    "        self.outputs.insert(0, self.activate(\n",
    "            np.dot(self.layers[0], instance.T)))\n",
    "\n",
    "        # loop for hidden and output layers calculating the output and passing to next layer\n",
    "        for layer in range(1, self.no_hidden_layers+2):\n",
    "            # get the weighted sum of inputs for each layer and pass to activation function\n",
    "            # then store as output of the layer\n",
    "            self.outputs.insert(layer, self.activate(\n",
    "                np.dot(self.layers[layer], np.hstack(([1], self.outputs[layer-1])))\n",
    "            ))       \n",
    "    \n",
    "        # return the final output from the network, i.e. the prediction\n",
    "        return self.outputs[-1]\n",
    "    \n",
    "    #===================================#\n",
    "    # Performs the backprop phase of    #\n",
    "    # the Neural network, returns the   #\n",
    "    # Partial derivatives.              #\n",
    "    #===================================#  \n",
    "    def back_propagation(self, inputs, x, y, i, predicted):\n",
    "        partial_derivatives = [0] * (self.no_hidden_layers+2)\n",
    "        error_terms = [0] * (self.no_hidden_layers+2)\n",
    "        inputs_w_bias = np.hstack(([1], inputs))\n",
    "        \n",
    "        # for layer in layers reversed\n",
    "        for l in range(self.no_hidden_layers+1, -1, -1):\n",
    "            # if output layer\n",
    "            if l == self.no_hidden_layers+1:\n",
    "                error_terms.insert(l, self.loss(predicted, y.iloc[i])*self.derivative(predicted))\n",
    "                \n",
    "            # if input layer or hidden layer\n",
    "            else:\n",
    "                # for neuron in next layer get weight of this neuron * error * derivative of activation function\n",
    "                layer_errors = []\n",
    "                for j in range(len(self.layers[l])):\n",
    "                    error = 0.0\n",
    "                    for k in range(len(self.layers[l+1])):\n",
    "                        error += self.layers[l+1][k][j] *error_terms[l+1][k] * self.derivative(self.outputs[l][j])\n",
    "                    layer_errors.insert(j, error)\n",
    "                \n",
    "                # store the errors for this layer\n",
    "                error_terms.insert(l, np.array(layer_errors))\n",
    "                \n",
    "            # calculate partial derivative for this layer,\n",
    "            # if input layer use x, else use previous layers output\n",
    "            if l == 0:\n",
    "                partial_derivatives[l] = np.dot(error_terms[l][np.newaxis,:].T, inputs_w_bias[np.newaxis,:])\n",
    "            else:\n",
    "                inputs_t_layer = np.hstack(([1], self.outputs[l-1]))[np.newaxis,:]\n",
    "                partial_derivatives[l] = np.dot(error_terms[l][np.newaxis,:].T, inputs_t_layer) \n",
    "                \n",
    "        # return the partial derivatives       \n",
    "        return partial_derivatives\n",
    "\n",
    "    #===============================#\n",
    "    # Trains the net using labelled #\n",
    "    # training data.                #\n",
    "    #===============================#\n",
    "    def train(self, training_data, labels, target):\n",
    "        # time how long it takes to train\n",
    "        start_time = time.time()\n",
    "        # predicting parameter represents which digit we will be \n",
    "        # predicting in a OneVsRest style\n",
    "        self.target = target\n",
    "        \n",
    "        # make sure the length of the x and y sets\n",
    "        # are the same, i.e. no missing data\n",
    "        assert len(training_data) == len(labels)\n",
    "        \n",
    "        # group all Y values that are not equal to\n",
    "        # the class we are trying to predict and set\n",
    "        # them to 0 and the target to 1\n",
    "        labels = np.where(labels == self.target, 1, 0)\n",
    "        \n",
    "        # perform the training\n",
    "        print(\"Training model...\")\n",
    "        for _ in range(self.max_iterations):\n",
    "            \n",
    "            # shuffle both x and y together to perform stochastic gradient descent\n",
    "            # this is using a reindexing method to ensure they maintain \n",
    "            # the correct pairs of predictors and labels\n",
    "            idx = np.random.permutation(training_data.index)\n",
    "            x = training_data.reindex(idx).reset_index(drop=True)\n",
    "            y = pd.DataFrame(labels).reindex(idx).reset_index(drop=True)\n",
    "            \n",
    "            for i in range(len(y)):\n",
    "                # get the forward phase prediction for the instance of training data (x)\n",
    "                inputs = x.iloc[i]\n",
    "                predicted = self.forward_phase(inputs)\n",
    "                \n",
    "                # backprop to find partial derivatives\n",
    "                partial_derivatives = self.back_propagation(inputs, x, y, i, predicted)\n",
    "\n",
    "                # now update the weights for each layer\n",
    "                for layer in range(self.no_hidden_layers+2):\n",
    "                    self.layers[layer] = self.layers[layer] - (np.array(partial_derivatives[layer])*self.learning_rate)\n",
    "                    \n",
    "            print(\"Epoch {}/{} completed.\".format(_+1, self.max_iterations))\n",
    "        print(\"Model trained.\")\n",
    "        print ('Time elapsed: {:.4f} seconds'.format(time.time()-start_time))\n",
    "\n",
    "    #=========================================#\n",
    "    # Function to report on test results      #\n",
    "    #=========================================#\n",
    "    def class_report(self, actuals, preds):\n",
    "        # init a confusion matrix\n",
    "        confusion_matrix = {\"tn\":0, \"tp\":0, \"fp\":0, \"fn\":0}\n",
    "        \n",
    "        # for each pred check result vs actual to construct the matrix\n",
    "        for actual, pred in zip(actuals, preds):\n",
    "            if (pred == actual):\n",
    "                if pred == 1:\n",
    "                    confusion_matrix[\"tp\"] +=1\n",
    "                else:\n",
    "                    confusion_matrix[\"tn\"] +=1\n",
    "            else:\n",
    "                if pred == 1:\n",
    "                    confusion_matrix[\"fp\"] +=1\n",
    "                else:\n",
    "                    confusion_matrix[\"fn\"] +=1\n",
    "                    \n",
    "        # use try except blocks to catch any possible divisions by 0 in calculations\n",
    "        try:\n",
    "            # calculate accuracy, recall, precision and f1 based on confusion matrix\n",
    "            accuracy = ((confusion_matrix[\"tp\"] + confusion_matrix[\"tn\"])/\n",
    "                       (confusion_matrix[\"tp\"] + confusion_matrix[\"tn\"] + confusion_matrix[\"fp\"] + confusion_matrix[\"fn\"]))*100\n",
    "\n",
    "            precision = confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fp\"])*100\n",
    "\n",
    "            recall =  confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fn\"])*100\n",
    "\n",
    "            f1 = 2*((precision*recall)/(precision+recall))\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            print(\"Error occured, tried to divide by 0. Model predicted either no Tp or no Tn\")\n",
    "        \n",
    "        try:\n",
    "            # output results\n",
    "            print(\"Testing Results\")\n",
    "            print(\"----------------------------\")\n",
    "            print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "            print(\"Precision: {:.2f}%\".format(precision))\n",
    "            print(\"Recall: {:.2f}%\".format(recall))\n",
    "            print(\"F1 Score: {:.2f}%\".format(f1))\n",
    "            print(\"Confusion Matrix: {}\".format(\n",
    "                    [[confusion_matrix[\"tp\"], confusion_matrix[\"fp\"]],\n",
    "                    [confusion_matrix[\"fn\"], confusion_matrix[\"tn\"]]]\n",
    "                ))\n",
    "            print(\"----------------------------\")\n",
    "        except UnboundLocalError:\n",
    "            pass\n",
    "        \n",
    "    #=========================================#\n",
    "    # Tests the prediction on each element of #\n",
    "    # the testing data. Prints the precision, #\n",
    "    # recall, and accuracy.                   #\n",
    "    #=========================================#\n",
    "    def test(self, testing_data, labels):\n",
    "        # make sure the length of the x and y sets\n",
    "        # are the same, i.e. no missing data\n",
    "        assert len(testing_data) == len(labels)\n",
    "        \n",
    "        # group all Y values that are not equal to\n",
    "        # the class we are trying to predict and set\n",
    "        # them to 0 and the target to 1\n",
    "        labels = np.where(labels == self.target, 1, 0)\n",
    "        \n",
    "        preds = np.ones(shape=len(labels))\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            instance = testing_data.iloc[i]\n",
    "            # add 1 to training data for use with bias\n",
    "            # get prediction and append to the preds list\n",
    "            preds[i] = self.forward_phase(instance)\n",
    "        \n",
    "        # use threshold of 0.5\n",
    "        preds = np.where(preds >= 0.5, 1, 0)\n",
    "        \n",
    "        # output the results\n",
    "        self.class_report(labels, preds)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the MNIST data\n",
    "train_data = pd.read_csv(\"./mnist_train.csv\", header=None)\n",
    "test_data = pd.read_csv(\"./mnist_test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate into X and Y for train and test\n",
    "# divide by 255 to scale the data between 0 and 1\n",
    "X_train = train_data.drop(train_data.columns[0], axis=1) /255\n",
    "Y_train = train_data[train_data.columns[0]].copy()\n",
    "\n",
    "X_test = test_data.drop(train_data.columns[0], axis=1) /255\n",
    "Y_test = test_data[train_data.columns[0]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of Main Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "----------------------------\n",
      "No. inputs: 784\n",
      "Hidden Layers: (1,16)\n",
      "Max iterations:\t1\n",
      "Learning rate:\t0.1\n",
      "Activation function: sigmoid\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialise a neural net and view the details, \n",
    "nn = ANN(28*28, max_iterations=1)\n",
    "nn.print_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/1 completed.\n",
      "Model trained.\n",
      "Time elapsed: 46.8488 seconds\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train, Y_train, target = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results\n",
      "----------------------------\n",
      "Accuracy: 99.38%\n",
      "Precision: 97.94%\n",
      "Recall: 96.56%\n",
      "F1 Score: 97.25%\n",
      "Confusion Matrix: [[1096, 23], [39, 8842]]\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "nn.test(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Complete initilisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary\n",
      "----------------------------\n",
      "No. inputs: 784\n",
      "Hidden Layers: (2,8)\n",
      "Max iterations:\t3\n",
      "Learning rate:\t0.2\n",
      "Activation function: sigmoid\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialise a neural net and view the details, \n",
    "nn2 = ANN(28*28, max_iterations=3, learning_rate = 0.2, no_hidden_layers=2, hidden_layer_size=8, output_layer_size=1)\n",
    "nn2.print_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/3 completed.\n",
      "Epoch 2/3 completed.\n",
      "Epoch 3/3 completed.\n",
      "Model trained.\n",
      "Time elapsed: 110.9935 seconds\n"
     ]
    }
   ],
   "source": [
    "nn2.train(X_train, Y_train, target = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results\n",
      "----------------------------\n",
      "Accuracy: 99.58%\n",
      "Precision: 98.58%\n",
      "Recall: 97.71%\n",
      "F1 Score: 98.14%\n",
      "Confusion Matrix: [[1109, 16], [26, 8849]]\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "nn2.test(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
